{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is a jupyter notebook which contains code for the artificial intelligence course for leaders. A Jupyter notebook is a series of cells which can be executed such that the code in them is run. In this course we will explore a data set by plotting with matplotlib. Predictive supervised models will be built, first regression models, then an artificial neural network. We will be following the machine learning pipeline outlined in the theory part of the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Following cell if the notebook is opened in Google Collab. It will clone the github repository to get all necessary files. To run a cell, mark it and press the \"Run\" button in the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NordAxon/AI-For-Leaders.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "Import all the libraries we need to run the code and perform the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Import Raw Data\n",
    "The data we will be using is a data set of housing sales in Melbourne. Each row is a sale with the columns being the features of the sold house/appartment. The goal is to predict the price of new sales based on the different features in the data set. \n",
    "Data source: https://www.kaggle.com/anthonypino/melbourne-housing-market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df_original = pd.read_csv('AI-For-Leaders/data/melbourne-housing-market/Melbourne_housing.csv')\n",
    "housing_df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the data frame\n",
    "housing_df_original.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Empty Data\n",
    "In some of the columns in the data we can see some entries are NaN, that means Not a Number. This is missing data. Many machine learning algorithms need data in all rows and columns so the NaNs have to be filled with something meaningful. That might be different for different columns. For example \"Landsize\" will be filled with 0 since a NaN it that case can be assumed to mean that there is no land contained in the real estate sale. Let's first have a look at how many NaN values are present in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of NaNs')\n",
    "print(pd.isnull(housing_df_original).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill up some of the missing data. \n",
    "- Remove rows where there is no price\n",
    "- For a couple of rows, fill with 0\n",
    "- For YearBuilt, fill with the mean of that column since the house being built aat year 0 seems unlikely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df_no_nan = housing_df_original.copy()\n",
    "\n",
    "# Remove all rows with no price data\n",
    "housing_df_no_nan = housing_df_no_nan[pd.notnull(housing_df_no_nan['Price'])]\n",
    "\n",
    "# Fill some rows with 0 if there is missing data\n",
    "housing_df_no_nan['BuildingArea'].fillna(0.0, inplace=True)\n",
    "housing_df_no_nan['Rooms'].fillna(0.0, inplace=True)\n",
    "housing_df_no_nan['Landsize'].fillna(0.0, inplace=True)\n",
    "housing_df_no_nan['Car'].fillna(0.0, inplace=True)\n",
    "housing_df_no_nan['Bathroom'].fillna(0.0, inplace=True)\n",
    "housing_df_no_nan['Bedroom2'].fillna(0.0, inplace=True)\n",
    "\n",
    "# Fill the missing \n",
    "housing_df_no_nan['YearBuilt'].fillna(housing_df_no_nan['YearBuilt'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Variable Correlations and Histograms\n",
    "- Let's have a look at how different variables relate to each other and how the data is distributed. We pick a couple of columns(variables) we think might be good predictors of price. These columns are plotted as scatter plots against each other and as histograms along the diagonal. \n",
    "- The goal is to get a feel for the data, understand how different columns relate to each other and look for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(housing_df_no_nan[['Price', 'BuildingArea', 'Rooms', 'Landsize', 'Car', 'Bathroom', 'Regionname']], \n",
    "                 hue=\"Regionname\", diag_kind='hist')\n",
    "housing_df_no_nan[['Price', 'BuildingArea', 'Rooms', 'Landsize', 'Car', 'Bathroom', 'Regionname']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outliers\n",
    "Outliers are data point which are located far from the other data points, these come with a risk of skewing the models and therefore we want to remove these. In the plot above it seems like we have outliers in some of the columns. We will have a deeper look at some columns which seem to contain outliers. Run the cell below to see a plot of price, here it is clear that we have some data points which are far away from the others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df_no_nan.plot.scatter('BuildingArea', 'Price', title='Price vs. Building Area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set a clip-off att 500 for building area which means that all data points larger than 500 will be set to 500 instead. A clip-off at 4000000 is set for price. The data is plotted again after the outliers are clipped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_clipped = housing_df_no_nan.copy()\n",
    "housing_clipped['BuildingArea'] = housing_clipped['BuildingArea'].clip(0, 500)\n",
    "housing_clipped['Price'] = housing_clipped['Price'].clip(0, 4e6)\n",
    "housing_clipped.plot.scatter('BuildingArea', 'Price', title='Price vs. Building Area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1: \n",
    "-  Plot \"Rooms\" vs \"Price\" in the following empty cell to see if there are any outliers\n",
    "-  If so, what could be a resonable cut-off?\n",
    "-  Clip the data set to remove Room outliers, i.e replace the value of variable rooms_max with a resonable number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out or clip outliers\n",
    "rooms_max = 200\n",
    "housing_clipped_r = housing_clipped.copy()\n",
    "housing_clipped_r['Rooms'] = housing_clipped_r['Rooms'].clip(0, rooms_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Histograms of Interesting Data Columns\n",
    "Look more closely at some of the variables we think could be interesting by plotting larger histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_clipped_r['BuildingArea'].hist(bins=40, figsize=(10,7))\n",
    "plt.title('Histogram of Building Areas');\n",
    "# TODO: Make it better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2: Create a histogram of Price in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price for Different Regions\n",
    "We are also hypothesising that the property location will have an impact on price. Below a plot will be made to get a feel for how different locations might affect the price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "housing_clipped_r.groupby('Regionname')['Price'].mean().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3: Plot Price for Different CouncilArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple One Dimensional Linear Regression\n",
    "We are getting a feel for what the data looks like, so now we might try a first model for predicting price. The linear regression is a simple but very commonly used model. We pick the building area as a predictor to begin with since there seem to be a correlation between building area and price according to our exploration. \n",
    "\n",
    "\n",
    "# 4) Model Training\n",
    "\n",
    "\n",
    "\n",
    "We'll start of by looking at a subset of the housing data, only in the council area of Yarra.\n",
    "The goal is to find all of the weights, $w_i$, in the following linear regression model. \n",
    "$y = w_0 + w_1x_1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up input and output variables\n",
    "y = housing_clipped_r['Price']\n",
    "x = housing_clipped_r[['BuildingArea']]\n",
    "\n",
    "# Split into test and train data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n",
    "\n",
    "# Set up regression model\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the regression model\n",
    "regr.fit(x_train, y_train)\n",
    "\n",
    "# Perform predictions\n",
    "y_pred = regr.predict(x_test)\n",
    "\n",
    "# Print regression coefficients, w\n",
    "print(regr.coef_)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15,10));\n",
    "plt.plot(x_test, y_pred, 'r');\n",
    "plt.scatter(x_test.values, y_test.values, alpha=0.1);\n",
    "plt.title('Simple Linear Regression Model');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Model Evaluation\n",
    "The error value will be compared to a baseline error which is the error if the prediction is just the mean of previous house values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Results\n",
    "mean_error = (y_pred - y_test).abs().mean()\n",
    "mean_error_baseline = (y_train.mean() - y_test).abs().mean()\n",
    "print('\\nBaseline Mean Error: ' + str(mean_error_baseline))\n",
    "print('Model Mean Absolute Error: ' + str(mean_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Dimensional Linear Regression\n",
    "# 4) Model Training\n",
    "In order to increase the predictive power, i.e. to get a more accurate model, more information can be added to the model. One way of doing that is by adding more input variables to the model. Variables that could be tried are BuildingArea, Rooms, LandSize, Car. <br><br>\n",
    "$y = w_0 + w_1x_1 + w_2x_2 + \\dots$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['BuildingArea', 'Rooms', 'Landsize', 'Car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = housing_clipped_r[feature_list]\n",
    "y = housing_clipped_r['Price']\n",
    "\n",
    "# Split into test and train data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n",
    "\n",
    "# Set up regression model\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the regression model\n",
    "regr.fit(x_train, y_train)\n",
    "\n",
    "# Perform predictions\n",
    "y_pred = regr.predict(x_test)\n",
    "\n",
    "# Print regression coefficients, w\n",
    "print('Regression Coefficients, w0, w1, w2, ...')\n",
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Results\n",
    "mean_error = (y_pred - y_test).abs().mean()\n",
    "print('\\nMean Error Multi-Dimensional Linear Regression: ' + str(mean_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4: Add the data column 'Car' to the input data and see if your results change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "The code in the following cell transforms data, builds a neural network and evaluates results of predictions from the neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpy.random import seed\n",
    "#seed(1)\n",
    "# Filter out the wanted columns\n",
    "\n",
    "def run_neural_network(x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    y_train = scaler.fit_transform(y_train.values.reshape(len(y_train),1))\n",
    "    y_test = scaler.transform(y_test.values.reshape(len(y_test),1))\n",
    "\n",
    "    # define the neural network structure\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=x_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "    # train the model\n",
    "    history = model.fit(x_train, y_train, epochs=15, verbose=0, validation_data=(x_test, y_test))\n",
    "\n",
    "    # make a prediction\n",
    "    y_pred = model.predict(x_test)[:,0]\n",
    "    # show the inputs and predicted outputs\n",
    "\n",
    "    y_pred = scaler.inverse_transform(y_pred.reshape(len(y_pred),1))\n",
    "    y_test = scaler.inverse_transform(y_test)\n",
    "\n",
    "    # Evaluate Results\n",
    "    mean_error = (pd.Series(y_pred[:,0]) - y_test[:,0]).abs().mean()\n",
    "    print('Mean Absolute Test Error: ' + str(mean_error))\n",
    "\n",
    "    # Plot error over training time\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.plot(history.history['mean_squared_error'])\n",
    "    plt.plot(history.history['val_mean_squared_error'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "run_neural_network(housing_clipped_r[feature_list], housing_clipped_r['Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 5: Try different number of training epochs\n",
    "-  What happens with the loss for a higher amount of training epochs(time)?\n",
    "\n",
    "### Assignment 6: Try different sizes of the network\n",
    "-  What are the results when more layers are added? \n",
    "-  What are the results when more neurons in each layer is added.\n",
    "-  Why is there a difference between train and test data in absolute error? \n",
    "\n",
    "### Assignment 7: Add more features\n",
    "-  Which columns could be useful for providing more predictive power?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "-  Which of the different models performed best? Why?\n",
    "-  Why does a well tuned neural network perform better than a linear regression model?\n",
    "-  What could be done to increase predictive power?\n",
    "-  Which additional data do you think would make a large differce in predictive power?\n",
    "-  What was the lowest mean square error you got?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Assignment:\n",
    "## Add More Features\n",
    "Let's look at the data to see what we can do with the data to create columns which are more easily readable for a machine learning algorithm and how we can provide more information from the data we have. \n",
    "Try to make the predictive model as good as possible by adding more features such as location and age of property. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One-hot encoding of Region\n",
    "one_hot_region = pd.get_dummies(housing_clipped_r.Regionname, prefix='Regionname')\n",
    "housing_df_feature = pd.concat([housing_clipped_r, one_hot_region], axis=1)\n",
    "one_hot_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 8: Modify above code to include CouncilArea in addition to Regionname "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add house age as a feature\n",
    "Using the build year of a house directly as a feature is not good since most values will be around 2000. A small difference in feature, might be a big difference in actual house value. E.g. a house built 2017 is probably alot more valuable than a house build 2007, but that is still a small percentual difference. The percentual difference between the age of 1 year and 11 years on the other hand gives a large difference. We also log-transform the age to make it a bit more convenient for machine learning algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2018. - housing_df_feature['YearBuilt']).apply(lambda x: np.log(x))\n",
    "housing_df_feature['YearBuilt'] = housing_df_feature['YearBuilt'].clip(0, 2018)\n",
    "house_ages = pd.Series(np.log((1 + (2018. - housing_df_feature['YearBuilt']))))\n",
    "(2018. - housing_df_feature['YearBuilt']).hist(bins=30)\n",
    "plt.title('Distribution before logarithm')\n",
    "housing_df_feature['Age'] = house_ages\n",
    "plt.figure()\n",
    "house_ages.hist()\n",
    "plt.title('Distribution After logarithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Neural Network with New Features\n",
    "With new features added it is time to train the neural network again, run the next cell and check the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature_list = ['BuildingArea', 'Rooms', 'Landsize', 'Car', 'Age'] + list(one_hot_region.columns)\n",
    "\n",
    "run_neural_network(housing_df_feature[new_feature_list], housing_df_feature['Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 9\n",
    "- Check the original data to see if there are other features(columns) which might generate even better predictions if they are part of the model. \n",
    "- Write code below to add these new features and train a new network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_course_env",
   "language": "python",
   "name": "course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
